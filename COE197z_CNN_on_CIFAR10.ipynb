{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COE197z_CNN_on_CIFAR10.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "MxX-XaFic-zV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1037
        },
        "outputId": "9543de06-12e5-45ac-d6c3-96a571f23987"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Activation\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#CNN ON CIFAR\n",
        "\n",
        "# load CIFAR dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# from sparse label to categorical\n",
        "num_labels = len(np.unique(y_train))\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# reshape and normalize input images\n",
        "image_size = x_train.shape[1]\n",
        "channels = 3\n",
        "input_size = image_size * image_size * channels\n",
        "x_train = np.reshape(x_train,[-1, image_size, image_size, 3])\n",
        "x_test = np.reshape(x_test,[-1, image_size, image_size, 3])\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# image is processed as is\n",
        "input_shape = (image_size, image_size, 3)\n",
        "batch_size = 128\n",
        "kernel_size = 3\n",
        "pool_size = 2\n",
        "filters = 64\n",
        "dropout = 0.2\n",
        "data_augmentation = True\n",
        "max_batches = len(x_train) / batch_size\n",
        "epochs = 25\n",
        "\n",
        "# model is a stack of CNN-ReLU-MaxPooling\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=filters,\n",
        "                 kernel_size=kernel_size,\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(filters=filters,\n",
        "                 kernel_size=kernel_size,\n",
        "                 activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(filters=filters,\n",
        "                 kernel_size=kernel_size,\n",
        "                 activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('relu'))\n",
        "# dropout added as regularizer\n",
        "model.add(Dropout(dropout))\n",
        "# output layer is 10-dim one-hot vector\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "plot_model(model, to_file='cnn-mnist.png', show_shapes=True)\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of adam optimizer\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    # train the network no data augmentation\n",
        "    x_train = np.reshape(x_train, [-1, input_size])\n",
        "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    # we need [width, height, channel] dim for data aug\n",
        "    x_train = np.reshape(x_train, [-1, image_size, image_size, 3])\n",
        "    datagen = ImageDataGenerator(\n",
        "        data_format=\"channels_last\",\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=0.0,  # randomly rotate images in the range (deg 0 to 180)\n",
        "        width_shift_range=0.0,  # randomly shift images horizontally\n",
        "        height_shift_range=0.0,  # randomly shift images vertically\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "    datagen.fit(x_train)\n",
        "    for e in range(epochs):\n",
        "        print(\"Epoch: %d/%d\" % (e+1, epochs))\n",
        "        batches = 0\n",
        "        for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=batch_size):\n",
        "            x_batch = np.reshape(x_batch, [-1, image_size, image_size, 3])\n",
        "            model.fit(x_batch, y_batch, verbose=0)\n",
        "            batches += 1\n",
        "            if batches >= max_batches:\n",
        "                # we need to break the loop by hand because\n",
        "                # the generator loops indefinitely\n",
        "                break\n",
        "\n",
        "# Score trained model.\n",
        "x_test = np.reshape(x_test, [-1, image_size, image_size, 3])\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_54 (Conv2D)           (None, 30, 30, 64)        1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_40 (MaxPooling (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_55 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_41 (MaxPooling (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_56 (Conv2D)           (None, 4, 4, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_17 (Flatten)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 10)                10250     \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 85,898\n",
            "Trainable params: 85,898\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "Epoch: 1/25\n",
            "Epoch: 2/25\n",
            "Epoch: 3/25\n",
            "Epoch: 4/25\n",
            "Epoch: 5/25\n",
            "Epoch: 6/25\n",
            "Epoch: 7/25\n",
            "Epoch: 8/25\n",
            "Epoch: 9/25\n",
            "Epoch: 10/25\n",
            "Epoch: 11/25\n",
            "Epoch: 12/25\n",
            "Epoch: 13/25\n",
            "Epoch: 14/25\n",
            "Epoch: 15/25\n",
            "Epoch: 16/25\n",
            "Epoch: 17/25\n",
            "Epoch: 18/25\n",
            "Epoch: 19/25\n",
            "Epoch: 20/25\n",
            "Epoch: 21/25\n",
            "Epoch: 22/25\n",
            "Epoch: 23/25\n",
            "Epoch: 24/25\n",
            "Epoch: 25/25\n",
            "10000/10000 [==============================] - 9s 914us/step\n",
            "Test loss: 0.7358111720085144\n",
            "Test accuracy: 0.7554\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}