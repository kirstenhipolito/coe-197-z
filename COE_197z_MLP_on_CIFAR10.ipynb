{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COE_197z_MLP_on_CIFAR10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "SNFZ7TiSiTSi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14314
        },
        "outputId": "c30eec97-fb04-4b81-8357-46c2d82b16a5"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten\n",
        "from keras.utils import to_categorical, plot_model\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import cv2\n",
        "\n",
        "#MLP ON CIFAR\n",
        "\n",
        "# network parameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 25\n",
        "data_augmentation = True\n",
        "\n",
        "\n",
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "image_size = x_train.shape[1]\n",
        "# print('xtrain shape: ', x_train[0].shape)\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.imshow(x_train[0])\n",
        "# plt.show()\n",
        "# cv2.imshow(\"train\", x_train[0])\n",
        "# cv2.waitKey(0)\n",
        "# cv2.destroyAllWindows()\n",
        "x_train = x_train.reshape(-1, 3072)\n",
        "x_test = x_test.reshape(-1, 3072)\n",
        "x_train = x_train.astype('float32') / 255;\n",
        "x_test = x_test.astype('float32') / 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "input_size = 3072\n",
        "max_batches = len(x_train) / batch_size\n",
        "\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# model is a 3-layer MLP with ReLU and dropout after each layer\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_dim=input_size))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.40))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.08))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.12))\n",
        "model.add(Dense(num_classes))\n",
        "# this is the output for one-hot vector\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "plot_model(model, to_file='mlp-cifar.png', show_shapes=True)\n",
        "\n",
        "# loss function for one-hot vector\n",
        "# use of sgd optimizer with default lr=0.01\n",
        "# accuracy is good metric for classification tasks\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='Adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    # train the network no data augmentation\n",
        "    x_train = np.reshape(x_train, [-1, input_size])\n",
        "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    # we need [width, height, channel] dim for data aug\n",
        "    x_train = np.reshape(x_train, [-1, image_size, image_size, 3])\n",
        "    datagen = ImageDataGenerator(\n",
        "        data_format=\"channels_last\",\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=0.0,  # randomly rotate images in the range (deg 0 to 180)\n",
        "        width_shift_range=0.0,  # randomly shift images horizontally\n",
        "        height_shift_range=0.0,  # randomly shift images vertically\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "    datagen.fit(x_train)\n",
        "    for e in range(epochs):\n",
        "        print(\"Epoch: %d/%d\" % (e+1, epochs))\n",
        "        batches = 0\n",
        "        for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=batch_size):\n",
        "            x_batch = np.reshape(x_batch, [-1, input_size])\n",
        "            if e == 24:\n",
        "                model.fit(x_batch, y_batch, verbose=1)\n",
        "            else:\n",
        "                model.fit(x_batch, y_batch, verbose=0)\n",
        "            batches += 1\n",
        "            if batches >= max_batches:\n",
        "                # we need to break the loop by hand because\n",
        "                # the generator loops indefinitely\n",
        "                break\n",
        "\n",
        "# Score trained model.\n",
        "x_test = np.reshape(x_test, [-1, input_size])\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 3072)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_137 (Dense)            (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "activation_137 (Activation)  (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_138 (Dense)            (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "activation_138 (Activation)  (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_77 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_139 (Dense)            (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "activation_139 (Activation)  (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_78 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_140 (Dense)            (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_140 (Activation)  (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 2,103,818\n",
            "Trainable params: 2,103,818\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "Epoch: 1/25\n",
            "Epoch: 2/25\n",
            "Epoch: 3/25\n",
            "Epoch: 4/25\n",
            "Epoch: 5/25\n",
            "Epoch: 6/25\n",
            "Epoch: 7/25\n",
            "Epoch: 8/25\n",
            "Epoch: 9/25\n",
            "Epoch: 10/25\n",
            "Epoch: 11/25\n",
            "Epoch: 12/25\n",
            "Epoch: 13/25\n",
            "Epoch: 14/25\n",
            "Epoch: 15/25\n",
            "Epoch: 16/25\n",
            "Epoch: 17/25\n",
            "Epoch: 18/25\n",
            "Epoch: 19/25\n",
            "Epoch: 20/25\n",
            "Epoch: 21/25\n",
            "Epoch: 22/25\n",
            "Epoch: 23/25\n",
            "Epoch: 24/25\n",
            "Epoch: 25/25\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 300us/step - loss: 1.2833 - acc: 0.4844\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 282us/step - loss: 1.2942 - acc: 0.4766\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 285us/step - loss: 1.4867 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 302us/step - loss: 1.1087 - acc: 0.5938\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 289us/step - loss: 1.2843 - acc: 0.5156\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 293us/step - loss: 1.2340 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 291us/step - loss: 1.2439 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 300us/step - loss: 1.1859 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 325us/step - loss: 1.2241 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 303us/step - loss: 1.2273 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.1236 - acc: 0.5859\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 289us/step - loss: 1.2246 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 311us/step - loss: 1.2881 - acc: 0.5156\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 306us/step - loss: 1.2521 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 293us/step - loss: 1.3586 - acc: 0.4453\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 290us/step - loss: 1.3097 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 295us/step - loss: 1.2027 - acc: 0.5938\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 286us/step - loss: 1.3571 - acc: 0.4922\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 348us/step - loss: 1.2964 - acc: 0.4766\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 288us/step - loss: 1.2141 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 300us/step - loss: 1.2733 - acc: 0.5703\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 300us/step - loss: 1.1680 - acc: 0.5781\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 319us/step - loss: 1.3607 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 296us/step - loss: 1.3433 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 312us/step - loss: 1.2454 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.1526 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 316us/step - loss: 1.1952 - acc: 0.5781\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 302us/step - loss: 1.2839 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 292us/step - loss: 1.4875 - acc: 0.4531\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 298us/step - loss: 1.1268 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 291us/step - loss: 1.2550 - acc: 0.5781\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 292us/step - loss: 1.3917 - acc: 0.4609\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 285us/step - loss: 1.3796 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 291us/step - loss: 1.2180 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 283us/step - loss: 1.1468 - acc: 0.5781\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 296us/step - loss: 1.1957 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 294us/step - loss: 1.1895 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 290us/step - loss: 1.2288 - acc: 0.6250\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 294us/step - loss: 1.2311 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 301us/step - loss: 1.0657 - acc: 0.6172\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 374us/step - loss: 1.4702 - acc: 0.4609\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 320us/step - loss: 1.4169 - acc: 0.4922\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 330us/step - loss: 1.1478 - acc: 0.5938\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 305us/step - loss: 1.3787 - acc: 0.5000\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 316us/step - loss: 1.1164 - acc: 0.5938\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 290us/step - loss: 1.4104 - acc: 0.5156\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 323us/step - loss: 1.2991 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 310us/step - loss: 1.2771 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.2769 - acc: 0.5703\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 313us/step - loss: 1.3361 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 301us/step - loss: 1.1632 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 294us/step - loss: 1.1446 - acc: 0.5859\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 305us/step - loss: 1.0978 - acc: 0.5938\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 313us/step - loss: 1.2859 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 305us/step - loss: 1.3900 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 323us/step - loss: 1.3397 - acc: 0.5000\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 305us/step - loss: 1.3291 - acc: 0.4609\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 325us/step - loss: 1.2325 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 347us/step - loss: 1.3224 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 314us/step - loss: 1.3765 - acc: 0.5000\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 325us/step - loss: 1.2559 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 332us/step - loss: 1.3522 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.1434 - acc: 0.5781\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 289us/step - loss: 1.1927 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 286us/step - loss: 1.3750 - acc: 0.5156\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 291us/step - loss: 1.1544 - acc: 0.6094\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 319us/step - loss: 1.2029 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 310us/step - loss: 1.2877 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 291us/step - loss: 1.3556 - acc: 0.5156\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 306us/step - loss: 1.2030 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 311us/step - loss: 1.3413 - acc: 0.4844\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 300us/step - loss: 1.1919 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 294us/step - loss: 1.2255 - acc: 0.5703\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 308us/step - loss: 1.0350 - acc: 0.6016\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 315us/step - loss: 1.3168 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 298us/step - loss: 1.1725 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 285us/step - loss: 1.1954 - acc: 0.5859\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 290us/step - loss: 1.1639 - acc: 0.6406\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 299us/step - loss: 1.3065 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 309us/step - loss: 1.2113 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 286us/step - loss: 1.3507 - acc: 0.5156\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 320us/step - loss: 1.2940 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 289us/step - loss: 1.1495 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 366us/step - loss: 1.3295 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 327us/step - loss: 1.4473 - acc: 0.5000\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 322us/step - loss: 1.2663 - acc: 0.4844\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 315us/step - loss: 1.2585 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 300us/step - loss: 1.1303 - acc: 0.6250\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 316us/step - loss: 1.1937 - acc: 0.5703\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 290us/step - loss: 1.1946 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 311us/step - loss: 1.1715 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 298us/step - loss: 1.2111 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 327us/step - loss: 1.2184 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 297us/step - loss: 1.1516 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 305us/step - loss: 1.2341 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.3038 - acc: 0.5859\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 295us/step - loss: 1.5103 - acc: 0.5000\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.2642 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 287us/step - loss: 1.2903 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 322us/step - loss: 1.2950 - acc: 0.4766\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 313us/step - loss: 1.4623 - acc: 0.4609\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 295us/step - loss: 1.0870 - acc: 0.6094\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 297us/step - loss: 1.1804 - acc: 0.5938\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 336us/step - loss: 1.1899 - acc: 0.5703\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 302us/step - loss: 1.1962 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 378us/step - loss: 1.3337 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 324us/step - loss: 1.2876 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 301us/step - loss: 1.1727 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 316us/step - loss: 1.3535 - acc: 0.5000\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 299us/step - loss: 1.1788 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 300us/step - loss: 1.2616 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 310us/step - loss: 1.1459 - acc: 0.6328\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.1898 - acc: 0.5859\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 294us/step - loss: 1.2320 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 290us/step - loss: 1.1749 - acc: 0.5703\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 308us/step - loss: 1.3604 - acc: 0.5000\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 288us/step - loss: 1.2063 - acc: 0.5859\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 297us/step - loss: 1.1502 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 287us/step - loss: 1.1678 - acc: 0.6172\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 293us/step - loss: 1.3386 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 308us/step - loss: 1.3370 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 291us/step - loss: 1.1878 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 303us/step - loss: 1.1513 - acc: 0.5781\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 314us/step - loss: 1.3518 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 317us/step - loss: 1.4046 - acc: 0.4922\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 314us/step - loss: 1.2551 - acc: 0.5781\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 337us/step - loss: 1.2796 - acc: 0.5156\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 382us/step - loss: 1.2386 - acc: 0.6094\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 308us/step - loss: 1.3075 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 310us/step - loss: 1.3041 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 293us/step - loss: 1.3297 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.1713 - acc: 0.5703\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 288us/step - loss: 1.2176 - acc: 0.6328\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.2274 - acc: 0.5781\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 285us/step - loss: 1.2639 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 310us/step - loss: 1.3353 - acc: 0.5156\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 288us/step - loss: 1.2530 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 296us/step - loss: 1.1046 - acc: 0.6250\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 311us/step - loss: 1.1137 - acc: 0.6172\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 315us/step - loss: 1.3286 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 313us/step - loss: 1.2527 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 312us/step - loss: 1.2384 - acc: 0.5781\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 326us/step - loss: 1.2527 - acc: 0.5703\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 317us/step - loss: 1.3332 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 341us/step - loss: 1.3450 - acc: 0.4844\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 290us/step - loss: 1.2818 - acc: 0.5703\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 313us/step - loss: 1.4884 - acc: 0.4688\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 343us/step - loss: 1.4053 - acc: 0.4844\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 308us/step - loss: 1.3464 - acc: 0.4922\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 383us/step - loss: 1.3323 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 291us/step - loss: 1.1947 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 325us/step - loss: 1.1713 - acc: 0.6406\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 298us/step - loss: 1.2144 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 302us/step - loss: 1.3772 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 291us/step - loss: 1.2480 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 298us/step - loss: 1.3581 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.3257 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 320us/step - loss: 1.3091 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 283us/step - loss: 1.2955 - acc: 0.5156\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 309us/step - loss: 1.2750 - acc: 0.5000\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.1993 - acc: 0.5703\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 325us/step - loss: 1.3053 - acc: 0.4922\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 330us/step - loss: 1.3255 - acc: 0.4844\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 304us/step - loss: 1.3304 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.1773 - acc: 0.5703\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.2166 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 315us/step - loss: 1.2689 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 305us/step - loss: 1.3405 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 317us/step - loss: 1.3234 - acc: 0.5000\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.4014 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 343us/step - loss: 1.3241 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 321us/step - loss: 1.2220 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 296us/step - loss: 1.1643 - acc: 0.6094\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 347us/step - loss: 1.1081 - acc: 0.5859\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 323us/step - loss: 1.2133 - acc: 0.5859\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 317us/step - loss: 1.1252 - acc: 0.6328\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 315us/step - loss: 1.1549 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 302us/step - loss: 1.1114 - acc: 0.5859\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 314us/step - loss: 1.1465 - acc: 0.5938\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 317us/step - loss: 1.1714 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 296us/step - loss: 1.1469 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 314us/step - loss: 1.2832 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 291us/step - loss: 1.0753 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 326us/step - loss: 1.2856 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 315us/step - loss: 1.2119 - acc: 0.5938\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 289us/step - loss: 1.4667 - acc: 0.4531\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 304us/step - loss: 1.0516 - acc: 0.6172\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 292us/step - loss: 1.2776 - acc: 0.5703\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 299us/step - loss: 1.2843 - acc: 0.5938\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 294us/step - loss: 1.1716 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 288us/step - loss: 1.0716 - acc: 0.5938\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 288us/step - loss: 1.3354 - acc: 0.5000\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 321us/step - loss: 1.2941 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 298us/step - loss: 1.2373 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 306us/step - loss: 1.1869 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 302us/step - loss: 1.2634 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 325us/step - loss: 1.4610 - acc: 0.4844\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 326us/step - loss: 1.3967 - acc: 0.5156\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 322us/step - loss: 1.2745 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 290us/step - loss: 1.4189 - acc: 0.4688\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 303us/step - loss: 1.1896 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 296us/step - loss: 1.1860 - acc: 0.6094\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.2068 - acc: 0.5703\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 299us/step - loss: 1.2884 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 309us/step - loss: 1.4695 - acc: 0.4922\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 309us/step - loss: 1.1881 - acc: 0.5938\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 292us/step - loss: 1.3281 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 303us/step - loss: 1.1251 - acc: 0.5938\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 293us/step - loss: 1.3822 - acc: 0.4531\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 326us/step - loss: 1.3511 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 299us/step - loss: 1.3290 - acc: 0.5156\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.2024 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 311us/step - loss: 1.3142 - acc: 0.5000\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 324us/step - loss: 1.1299 - acc: 0.5781\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 381us/step - loss: 1.3284 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 317us/step - loss: 1.2199 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 319us/step - loss: 1.3492 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 300us/step - loss: 1.3195 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 312us/step - loss: 1.3187 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 298us/step - loss: 1.3175 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 314us/step - loss: 1.1930 - acc: 0.6016\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 333us/step - loss: 1.3735 - acc: 0.4688\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 306us/step - loss: 1.1700 - acc: 0.6016\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.2293 - acc: 0.6094\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 308us/step - loss: 1.4679 - acc: 0.4844\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 304us/step - loss: 1.3069 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 303us/step - loss: 1.2364 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 289us/step - loss: 1.2583 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 318us/step - loss: 1.3816 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 303us/step - loss: 1.2661 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 297us/step - loss: 1.2180 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 306us/step - loss: 1.2951 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 289us/step - loss: 1.3480 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 331us/step - loss: 1.2956 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 312us/step - loss: 1.3738 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 319us/step - loss: 1.2980 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 365us/step - loss: 1.2995 - acc: 0.4766\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 312us/step - loss: 1.2151 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 311us/step - loss: 1.1947 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 315us/step - loss: 1.2868 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 288us/step - loss: 1.3664 - acc: 0.5156\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 300us/step - loss: 1.3849 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 291us/step - loss: 1.3525 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 297us/step - loss: 1.1640 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 309us/step - loss: 1.2725 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.3454 - acc: 0.4844\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 306us/step - loss: 1.2620 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 293us/step - loss: 1.2771 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 294us/step - loss: 1.2137 - acc: 0.5859\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 325us/step - loss: 1.1186 - acc: 0.5938\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 306us/step - loss: 1.3116 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 300us/step - loss: 1.2943 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 297us/step - loss: 1.2934 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 296us/step - loss: 1.1971 - acc: 0.5938\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 312us/step - loss: 1.1842 - acc: 0.5859\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 312us/step - loss: 1.2755 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 301us/step - loss: 1.2419 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 295us/step - loss: 1.3677 - acc: 0.4844\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 332us/step - loss: 1.1535 - acc: 0.6016\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 322us/step - loss: 1.4039 - acc: 0.5156\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 311us/step - loss: 1.3614 - acc: 0.4609\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 291us/step - loss: 1.3499 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 305us/step - loss: 1.3425 - acc: 0.4844\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 302us/step - loss: 1.3000 - acc: 0.4922\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 316us/step - loss: 1.2386 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 306us/step - loss: 1.2190 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 317us/step - loss: 1.2406 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 316us/step - loss: 1.2964 - acc: 0.5000\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 289us/step - loss: 1.1929 - acc: 0.6172\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 304us/step - loss: 1.1440 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 321us/step - loss: 1.2480 - acc: 0.4844\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 314us/step - loss: 1.2429 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 296us/step - loss: 1.3564 - acc: 0.4844\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.2021 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 316us/step - loss: 1.2593 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 324us/step - loss: 1.0973 - acc: 0.5781\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.3927 - acc: 0.4688\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.1821 - acc: 0.6016\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 318us/step - loss: 1.1927 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 359us/step - loss: 1.3598 - acc: 0.4922\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 304us/step - loss: 1.3137 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 303us/step - loss: 1.2783 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 314us/step - loss: 1.1015 - acc: 0.6250\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 317us/step - loss: 0.9799 - acc: 0.6484\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 319us/step - loss: 1.4473 - acc: 0.4922\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 302us/step - loss: 1.2183 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 282us/step - loss: 1.2418 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 313us/step - loss: 1.1179 - acc: 0.5781\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 294us/step - loss: 1.2594 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 305us/step - loss: 1.3141 - acc: 0.4688\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 292us/step - loss: 1.2063 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 305us/step - loss: 1.2155 - acc: 0.5859\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 308us/step - loss: 1.3536 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 308us/step - loss: 1.1839 - acc: 0.6328\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 302us/step - loss: 1.1641 - acc: 0.6016\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 294us/step - loss: 1.4207 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 295us/step - loss: 1.2837 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 296us/step - loss: 1.3650 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 304us/step - loss: 1.1821 - acc: 0.5703\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 306us/step - loss: 1.2719 - acc: 0.5781\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 326us/step - loss: 1.1339 - acc: 0.5781\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 433us/step - loss: 1.2058 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 302us/step - loss: 1.3516 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 306us/step - loss: 1.3937 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 337us/step - loss: 1.3464 - acc: 0.5781\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 296us/step - loss: 1.5242 - acc: 0.4141\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 331us/step - loss: 1.2615 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.1932 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 304us/step - loss: 1.2087 - acc: 0.5703\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 308us/step - loss: 1.3522 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 302us/step - loss: 1.2491 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 289us/step - loss: 1.4143 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 301us/step - loss: 1.2637 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 306us/step - loss: 1.4063 - acc: 0.4844\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 303us/step - loss: 1.1340 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 301us/step - loss: 1.3193 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 311us/step - loss: 1.2522 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 298us/step - loss: 1.2024 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 321us/step - loss: 1.3711 - acc: 0.4766\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 414us/step - loss: 1.2538 - acc: 0.5156\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 318us/step - loss: 1.5110 - acc: 0.4141\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 296us/step - loss: 1.4355 - acc: 0.4688\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 352us/step - loss: 1.4028 - acc: 0.4375\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 319us/step - loss: 1.2647 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 337us/step - loss: 1.1396 - acc: 0.5938\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 303us/step - loss: 1.3313 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 290us/step - loss: 1.2428 - acc: 0.5703\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 303us/step - loss: 1.1936 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 298us/step - loss: 1.3341 - acc: 0.4844\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 297us/step - loss: 1.4907 - acc: 0.5000\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 299us/step - loss: 1.3588 - acc: 0.5156\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 301us/step - loss: 1.1850 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 312us/step - loss: 1.2254 - acc: 0.5859\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 293us/step - loss: 1.3815 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 301us/step - loss: 1.1756 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 295us/step - loss: 1.1836 - acc: 0.5703\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 282us/step - loss: 1.1453 - acc: 0.5859\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 326us/step - loss: 1.2583 - acc: 0.5703\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 310us/step - loss: 1.2155 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 294us/step - loss: 1.3554 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 317us/step - loss: 1.1949 - acc: 0.5938\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 301us/step - loss: 1.3212 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 299us/step - loss: 1.4937 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 308us/step - loss: 1.3128 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 352us/step - loss: 1.3177 - acc: 0.5000\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 349us/step - loss: 1.1735 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 279us/step - loss: 1.4213 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 311us/step - loss: 1.3200 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 326us/step - loss: 1.2614 - acc: 0.5703\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 308us/step - loss: 1.3171 - acc: 0.5000\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 291us/step - loss: 1.1334 - acc: 0.6250\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 290us/step - loss: 1.3977 - acc: 0.5156\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 320us/step - loss: 1.3224 - acc: 0.4922\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 315us/step - loss: 1.3106 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 292us/step - loss: 1.3772 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 298us/step - loss: 1.1968 - acc: 0.5859\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 300us/step - loss: 1.2977 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 307us/step - loss: 1.2525 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 303us/step - loss: 1.2089 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 320us/step - loss: 1.2245 - acc: 0.5859\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 301us/step - loss: 1.3028 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 304us/step - loss: 1.2951 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 291us/step - loss: 1.2343 - acc: 0.5938\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 292us/step - loss: 1.2255 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 304us/step - loss: 1.1324 - acc: 0.5938\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 311us/step - loss: 1.1059 - acc: 0.6172\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 355us/step - loss: 1.2105 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 321us/step - loss: 1.2716 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 313us/step - loss: 1.1833 - acc: 0.5547\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 332us/step - loss: 1.4063 - acc: 0.5000\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 323us/step - loss: 1.1459 - acc: 0.5859\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 316us/step - loss: 1.3368 - acc: 0.4531\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 291us/step - loss: 1.4518 - acc: 0.4766\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 320us/step - loss: 1.4702 - acc: 0.4375\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 321us/step - loss: 1.3685 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 313us/step - loss: 1.4716 - acc: 0.4922\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 310us/step - loss: 1.2573 - acc: 0.5156\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 275us/step - loss: 1.4221 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 323us/step - loss: 1.2853 - acc: 0.5391\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 292us/step - loss: 1.2377 - acc: 0.5469\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 301us/step - loss: 1.2274 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 312us/step - loss: 1.1902 - acc: 0.5938\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 310us/step - loss: 1.4027 - acc: 0.4375\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 315us/step - loss: 1.2492 - acc: 0.5625\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 313us/step - loss: 1.3088 - acc: 0.5234\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 309us/step - loss: 1.2563 - acc: 0.5312\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 302us/step - loss: 1.3020 - acc: 0.5078\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 304us/step - loss: 1.4259 - acc: 0.4453\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 389us/step - loss: 1.3453 - acc: 0.4922\n",
            "Epoch 1/1\n",
            "128/128 [==============================] - 0s 296us/step - loss: 1.1779 - acc: 0.5859\n",
            "Epoch 1/1\n",
            "80/80 [==============================] - 0s 348us/step - loss: 1.2307 - acc: 0.5500\n",
            "10000/10000 [==============================] - 2s 224us/step\n",
            "Test loss: 1.5192957984924316\n",
            "Test accuracy: 0.4798\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y0K2zpaJ87oY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**MLP on CIFAR10**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*Test 1*\n",
        "\n",
        "128 | relu\n",
        "\n",
        "128 | relu\n",
        "\n",
        "128 | relu\n",
        "\n",
        "Optimizer: SGD\n",
        "\n",
        "Epoch20 accuracy: 0.5106\n",
        "\n",
        "Test accuracy: 48.7%\n",
        "\n",
        "---\n",
        "\n",
        "*Test 2*\n",
        "\n",
        "128 | relu\n",
        "\n",
        "128 | relu\n",
        "\n",
        "128 | relu\n",
        "\n",
        "Optimizer: Adam\n",
        "\n",
        "Epoch20 accuracy: 0.5704\n",
        "\n",
        "Test accuracy: 51.0%\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "*Test 3*\n",
        "\n",
        "386 | relu | dropout\n",
        "\n",
        "386 | relu\n",
        "\n",
        "386 | relu | dropout\n",
        "\n",
        "Optimizer: Adam\n",
        "\n",
        "Epoch20 accuracy: 0.5549\n",
        "\n",
        "Test accuracy: 50.5%\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**network parameters**\n",
        "\n",
        "batch_size = 128\n",
        "hidden_units = 390\n",
        "dropout = 0.16\n",
        "num_classes = 10\n",
        "epochs = 25\n",
        "\n",
        "Epoch 25/25\n",
        "50000/50000 [==============================] - 4s 87us/step - loss: 1.3945 - acc: 0.4975\n",
        "\n",
        "10000/10000 [==============================] - 2s 201us/step\n",
        "\n",
        "Test accuracy: 49.1%\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**network parameters**\n",
        "\n",
        "batch_size = 64\n",
        "hidden_units = 256\n",
        "dropout = 0.12\n",
        "num_classes = 10\n",
        "epochs = 25\n",
        "\n",
        "Epoch 25/25\n",
        "50000/50000 [==============================] - 7s 133us/step - loss: 1.4289 - acc: 0.4887\n",
        "10000/10000 [==============================] - 3s 253us/step\n",
        "\n",
        "Test accuracy: 48.4%\n",
        "\n",
        "---\n",
        "\n",
        "**network parameters**\n",
        "batch_size = 256\n",
        "hidden_units = 386\n",
        "dropout = 0.12\n",
        "num_classes = 10\n",
        "epochs = 25\n",
        "\n",
        "Epoch 25/25\n",
        "50000/50000 [==============================] - 2s 49us/step - loss: 1.2511 - acc: 0.5504\n",
        "10000/10000 [==============================] - 2s 231us/step\n",
        "\n",
        "Test accuracy: 52.0%\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "tApQ9HPHoYAt",
        "colab_type": "code",
        "outputId": "ca60b2eb-8837-4bfc-9b1f-817bf2d2ae12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1941
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
        "from keras.models import Model\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "\n",
        "#CNN ON CIFAR\n",
        "\n",
        "# load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# from sparse label to categorical\n",
        "num_labels = len(np.unique(y_train))\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# reshape and normalize input images\n",
        "image_size = x_train.shape[1]\n",
        "x_train = np.reshape(x_train,[-1, image_size, image_size, 3])\n",
        "x_test = np.reshape(x_test,[-1, image_size, image_size, 3])\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "#network parameters\n",
        "input_shape = (image_size, image_size, 3)\n",
        "batch_size = 32\n",
        "kernel_size = 3\n",
        "dropout = 0.38\n",
        "n_filters = 32\n",
        "\n",
        "# left branch of Y network\n",
        "left_inputs = Input(shape=input_shape)\n",
        "x = left_inputs\n",
        "filters = n_filters\n",
        "# 3 layers of Conv2D-Dropout-MaxPooling2D\n",
        "# number of filters doubles after each layer (32-64-128)\n",
        "for i in range(3):\n",
        "    x = Conv2D(filters=filters,\n",
        "               kernel_size=kernel_size,\n",
        "               padding='same',\n",
        "               activation='relu')(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "    x = MaxPooling2D()(x)\n",
        "    filters *= 2\n",
        "\n",
        "# right branch of Y network\n",
        "right_inputs = Input(shape=input_shape)\n",
        "y = right_inputs\n",
        "filters = n_filters\n",
        "# 3 layers of Conv2D-Dropout-MaxPooling2D\n",
        "# number of filters doubles after each layer (32-64-128)\n",
        "for i in range(3):\n",
        "    y = Conv2D(filters=filters,\n",
        "               kernel_size=kernel_size,\n",
        "               padding='same',\n",
        "               activation='relu',\n",
        "               dilation_rate=2)(y)\n",
        "    y = Dropout(dropout)(y)\n",
        "    y = MaxPooling2D()(y)\n",
        "    filters *= 2\n",
        "\n",
        "# merge left and right branches outputs\n",
        "y = concatenate([x, y])\n",
        "# feature maps to vector in preparation to connecting to Dense layer\n",
        "y = Flatten()(y)\n",
        "y = Dropout(dropout)(y)\n",
        "outputs = Dense(num_labels, activation='softmax')(y)\n",
        "\n",
        "# build the model in functional API\n",
        "model = Model([left_inputs, right_inputs], outputs)\n",
        "# verify the model using graph\n",
        "plot_model(model, to_file='cnn-y-network.png', show_shapes=True)\n",
        "# verify the model using layer text description\n",
        "model.summary()\n",
        "\n",
        "# classifier loss, Adam optimizer, classifier accuracy\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the model with input images and labels\n",
        "model.fit([x_train, x_train],\n",
        "          y_train, \n",
        "          validation_data=([x_test, x_test], y_test),\n",
        "          epochs=20,\n",
        "          batch_size=batch_size)\n",
        "\n",
        "# model accuracy on test dataset\n",
        "score = model.evaluate([x_test, x_test], y_test, batch_size=batch_size)\n",
        "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 29s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 32)   896         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 32)   896         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32, 32, 32)   0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32, 32, 32)   0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 32)   0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 32)   0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 16, 16, 64)   18496       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 16, 16, 64)   18496       max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 16, 16, 64)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 16, 16, 64)   0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 64)     0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 8, 8, 64)     0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 8, 8, 128)    73856       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 8, 8, 128)    73856       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 8, 8, 128)    0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 8, 8, 128)    0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 4, 4, 128)    0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 4, 4, 128)    0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 4, 4, 256)    0           max_pooling2d_3[0][0]            \n",
            "                                                                 max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 4096)         0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 4096)         0           flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           40970       dropout_7[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 227,466\n",
            "Trainable params: 227,466\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 38s 751us/step - loss: 1.4942 - acc: 0.4609 - val_loss: 1.5633 - val_acc: 0.5099\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 32s 638us/step - loss: 1.1404 - acc: 0.5950 - val_loss: 1.2508 - val_acc: 0.6590\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 32s 637us/step - loss: 1.0225 - acc: 0.6392 - val_loss: 1.2646 - val_acc: 0.5939\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 32s 639us/step - loss: 0.9499 - acc: 0.6671 - val_loss: 1.2018 - val_acc: 0.6259\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 32s 637us/step - loss: 0.9029 - acc: 0.6825 - val_loss: 1.0596 - val_acc: 0.6565\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 32s 640us/step - loss: 0.8664 - acc: 0.6961 - val_loss: 1.1722 - val_acc: 0.6037\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 32s 639us/step - loss: 0.8439 - acc: 0.7051 - val_loss: 1.1127 - val_acc: 0.6212\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 32s 638us/step - loss: 0.8265 - acc: 0.7106 - val_loss: 1.1288 - val_acc: 0.6036\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 32s 637us/step - loss: 0.8086 - acc: 0.7154 - val_loss: 1.0149 - val_acc: 0.6738\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 32s 639us/step - loss: 0.7946 - acc: 0.7212 - val_loss: 1.0223 - val_acc: 0.6454\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 32s 640us/step - loss: 0.7868 - acc: 0.7223 - val_loss: 1.0606 - val_acc: 0.6273\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 32s 638us/step - loss: 0.7732 - acc: 0.7289 - val_loss: 0.9840 - val_acc: 0.6671\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 32s 637us/step - loss: 0.7663 - acc: 0.7316 - val_loss: 1.0251 - val_acc: 0.6412\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 32s 638us/step - loss: 0.7557 - acc: 0.7344 - val_loss: 0.9379 - val_acc: 0.6947\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 32s 637us/step - loss: 0.7487 - acc: 0.7381 - val_loss: 0.9599 - val_acc: 0.6849\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 32s 640us/step - loss: 0.7391 - acc: 0.7420 - val_loss: 1.0002 - val_acc: 0.6519\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 32s 636us/step - loss: 0.7403 - acc: 0.7408 - val_loss: 1.0191 - val_acc: 0.6607\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 32s 638us/step - loss: 0.7267 - acc: 0.7475 - val_loss: 1.1017 - val_acc: 0.6149\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 32s 640us/step - loss: 0.7222 - acc: 0.7487 - val_loss: 1.0157 - val_acc: 0.6363\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 32s 645us/step - loss: 0.7189 - acc: 0.7462 - val_loss: 0.9676 - val_acc: 0.6630\n",
            "10000/10000 [==============================] - 2s 173us/step\n",
            "\n",
            "Test accuracy: 66.3%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zUZn9qKJ9voU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**CNN on CIFAR10**\n",
        "\n",
        "*Test 1 (Siamese Network)*\n",
        "\n",
        "128 | relu\n",
        "\n",
        "128 | relu\n",
        "\n",
        "128 | relu\n",
        "\n",
        "Optimizer: adam\n",
        "\n",
        "Epoch20 accuracy: 0.6630\n",
        "\n",
        "Test accuracy: 66.3%\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}